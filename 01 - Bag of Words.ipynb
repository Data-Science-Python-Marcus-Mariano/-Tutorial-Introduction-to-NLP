{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#fc5858\">Bag of Words Model</h1>\n",
    "\n",
    "Tutorial for improve skills: 'Introduction to NLP' (Normalized Nerd) by Marcus Mariano\n",
    "\n",
    "**For more information about Marcus Mariano: [Web site](https://marcusmariano.github.io/mmariano/)**  \n",
    "\n",
    "**Introduction to NLP [Normalized Nerd.](https://www.youtube.com/playlist?list=PLM8wYQRetTxCCURc1zaoxo9pTsoov3ipY)** \n",
    "\n",
    "**Introduction to NLP | Bag of Words Model [Normalized Nerd.](https://www.youtube.com/watch?v=8Mlc4-3tgzc&list=PLM8wYQRetTxCCURc1zaoxo9pTsoov3ipY&index=2&t=2s)** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words\n",
    "\n",
    "steps\n",
    "\n",
    "1. Lowercase\n",
    "- Remove punctuation to empty space\n",
    "- Root words (Stemming or Lemmatization)\n",
    "- Full words (eliminate contractions, abbreviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((748, 2), (1000, 2), (1000, 2))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_imdb = pd.read_csv(\"data/imdb_labelled.txt\", delimiter='\\t', header=None)\n",
    "data_imdb.columns = [\"Review_text\", \"Review_class\"]\n",
    "\n",
    "data_amazon = pd.read_csv(\"data/amazon_cells_labelled.txt\", delimiter='\\t', header=None)\n",
    "data_amazon.columns = [\"Review_text\", \"Review_class\"]\n",
    "\n",
    "data_yelp = pd.read_csv(\"data/yelp_labelled.txt\", delimiter='\\t', header=None)\n",
    "data_yelp.columns = [\"Review_text\", \"Review_class\"]\n",
    "\n",
    "data_imdb.shape, data_amazon.shape, data_yelp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review_text</th>\n",
       "      <th>Review_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sure who was more lost - the flat characte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very little music or anything to speak of.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The best scene in the movie was when Gerardo i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>I think food should have flavor and texture an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Appetite instantly gone.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Overall I was not impressed and would not go b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>The whole experience was underwhelming, and I ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Then, as if I hadn't wasted enough of my life ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2748 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Review_text  Review_class\n",
       "0    A very, very, very slow-moving, aimless movie ...             0\n",
       "1    Not sure who was more lost - the flat characte...             0\n",
       "2    Attempting artiness with black & white and cle...             0\n",
       "3         Very little music or anything to speak of.               0\n",
       "4    The best scene in the movie was when Gerardo i...             1\n",
       "..                                                 ...           ...\n",
       "995  I think food should have flavor and texture an...             0\n",
       "996                           Appetite instantly gone.             0\n",
       "997  Overall I was not impressed and would not go b...             0\n",
       "998  The whole experience was underwhelming, and I ...             0\n",
       "999  Then, as if I hadn't wasted enough of my life ...             0\n",
       "\n",
       "[2748 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = pd.concat([data_imdb, data_amazon, data_yelp])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(df):\n",
    "    all_reviews = list()\n",
    "    lines = df[\"Review_text\"].values.tolist()\n",
    "    for text in lines:\n",
    "        text = text.lower()\n",
    "        pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "        text = pattern.sub('', text)\n",
    "        text = re.sub(r\"[,.\\\"!@#$%^&*(){}?/;`~:<>+=-]\", \"\", text)\n",
    "        tokens = word_tokenize(text)\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        stop_words.discard(\"not\")\n",
    "        PS = PorterStemmer()\n",
    "#         words = [w for w in words if not w in stop_words]\n",
    "        words = [PS.stem(w) for w in words if not w in stop_words]\n",
    "        words = ' '.join(words)\n",
    "        all_reviews.append(words)\n",
    "    return all_reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['slowmov aimless movi distress drift young man',\n",
       " 'not sure lost flat charact audienc nearli half walk',\n",
       " 'attempt arti black white clever camera angl movi disappoint becam even ridicul act poor plot line almost nonexist',\n",
       " 'littl music anyth speak',\n",
       " 'best scene movi gerardo tri find song keep run head',\n",
       " 'rest movi lack art charm mean empti work guess empti',\n",
       " 'wast two hour',\n",
       " 'saw movi today thought good effort good messag kid',\n",
       " 'bit predict',\n",
       " 'love cast jimmi buffet scienc teacher',\n",
       " 'babi owl ador',\n",
       " 'movi show lot florida best made look appeal',\n",
       " 'song best muppet hilari',\n",
       " 'cool',\n",
       " 'right case movi deliv everyth almost right face',\n",
       " 'averag act main person low budget clearli see',\n",
       " 'review long overdu sinc consid tale two sister singl greatest film ever made',\n",
       " 'put gem movi term screenplay cinematographi act postproduct edit direct aspect filmmak',\n",
       " 'practic perfect true masterpiec sea faux masterpiec',\n",
       " 'structur film easili tightli construct histori cinema think film someth vital import occur everi minut word content level film enough easili fill dozen film anyon right mind ask anyth movi quit simpli highest superl form cinema imagin ye film requir rather signific amount puzzlesolv piec fit togeth creat beauti pictur short film certainli pull punch graphic far best part game number one best th game seri deserv strong love insan game massiv level massiv unlock charact massiv game wast money game kind money wast properli actual graphic good time today graphic crap say canada fun game aye game rock buy play enjoy love pure brillianc flick doom concept idea lame take minor charact mediocr film make complet nonsequel chang tone pgrate famili movi nt least bit interest not confirm film would unfunni gener also manag give away entir movi not exagger everi moment everi plot point everi joke told trailer not funni even talent carrel ca nt save costar nt fare much better peopl like morgan freeman jonah hill ed helm wast stori predict lazi real effect work presenc anim integr scene worst obviou bluegreenscreen work ever seen whatev cost much nt translat qualiti sure film succe despit perhap obvious meagr budget glad film nt go obviou choic lesser film certainli would addit one love song ever written french cancan also boast one cutest lead ladi ever grace screen hard not fall headoverheel love girl neg insipid enough caus regret anoth hour life wast front screen long whini pointless recommend wait futur effort let one go excel cast stori line perform total believ ann hech utterli convinc sam shepard portray gung ho marin sober sat rivet tv screen give one resound think tom hank good actor enjoy read book children littl disappoint movi one charact total annoy voic give feel fingernail chalkboard total unnecessari trainrol coaster scene absolut warmth charm scene charact movi total grate nerv perform not improv improvis actor twice much worri not whether deliv line well whether line good quit honestli often not good often dialogu nt realli follow one line anoth fit surround crackl unpredict youth energi honestli found hard follow concentr meander badli gener great thing would nt say worth hour time though suspens builder good cross line g pg especi like nonclich choic parent movi could predict dialog verbatim write movi made better select want movi not gross give chill great choic alexand nevski great film amaz film artist one import whoever live glad pretenti piec nt plan dodg stratu big shot gon na help movi maker nt restrain movi busi quÃ©bec']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_reviews = clean_text(data)\n",
    "all_reviews[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV = CountVectorizer(min_df=3)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2748, 1192)\n",
      "(2748,)\n"
     ]
    }
   ],
   "source": [
    "X = CV.fit_transform(all_reviews).toarray()\n",
    "# y = data.as_matrix([\"Review_class\"])\n",
    "y = data[\"Review_class\"]\n",
    "print(np.shape(X))\n",
    "print(np.shape(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2198, 1192) (550, 1192) (2198,) (550,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, \n",
    "                                                    test_size = 0.2, \n",
    "                                                    random_state = 0)\n",
    "\n",
    "print(X_train.shape, X_valid.shape, y_train.shape, y_valid.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# model = DecisionTreeClassifier(criterion=\"entropy\", random_state=41)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "y_pred = model.predict(X_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6618181818181819\n",
      "0.7038216560509554\n",
      "0.6314285714285715\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "print(accuracy_score(y_valid, y_pred))\n",
    "print(f1_score(y_valid, y_pred))\n",
    "print(precision_score(y_valid, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
